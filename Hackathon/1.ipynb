{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d024f0d5-f9e0-429a-8f93-2319630bd6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Banggood Scraper...\n",
      "\n",
      "Scraping category: Chargers\n",
      "Found 26 items.\n",
      "  - Page 2... "
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.123)\nStacktrace:\n\tGetHandleVerifier [0x0x12dfe43+66515]\n\tGetHandleVerifier [0x0x12dfe84+66580]\n\t(No symbol) [0x0x10cdc48]\n\t(No symbol) [0x0x10ac18d]\n\t(No symbol) [0x0x1141a4e]\n\t(No symbol) [0x0x115c4d9]\n\t(No symbol) [0x0x113afc6]\n\t(No symbol) [0x0x110c2ca]\n\t(No symbol) [0x0x110d154]\n\tGetHandleVerifier [0x0x1537353+2521315]\n\tGetHandleVerifier [0x0x15322d3+2500707]\n\tGetHandleVerifier [0x0x1307c94+229924]\n\tGetHandleVerifier [0x0x12f81f8+165768]\n\tGetHandleVerifier [0x0x12fecad+193085]\n\tGetHandleVerifier [0x0x12e8158+100072]\n\tGetHandleVerifier [0x0x12e82f0+100480]\n\tGetHandleVerifier [0x0x12d25aa+11066]\n\tBaseThreadInitThunk [0x0x775dfcc9+25]\n\tRtlGetAppContainerNamedObjectPath [0x0x777882ae+286]\n\tRtlGetAppContainerNamedObjectPath [0x0x7778827e+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 153\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# RUN SCRIPT\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m     scrape_banggood()\n",
      "Cell \u001b[1;32mIn[12], line 120\u001b[0m, in \u001b[0;36mscrape_banggood\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n\u001b[0;32m    122\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.stop();\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:483\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:458\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    455\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor)\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    459\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:233\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    231\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.123)\nStacktrace:\n\tGetHandleVerifier [0x0x12dfe43+66515]\n\tGetHandleVerifier [0x0x12dfe84+66580]\n\t(No symbol) [0x0x10cdc48]\n\t(No symbol) [0x0x10ac18d]\n\t(No symbol) [0x0x1141a4e]\n\t(No symbol) [0x0x115c4d9]\n\t(No symbol) [0x0x113afc6]\n\t(No symbol) [0x0x110c2ca]\n\t(No symbol) [0x0x110d154]\n\tGetHandleVerifier [0x0x1537353+2521315]\n\tGetHandleVerifier [0x0x15322d3+2500707]\n\tGetHandleVerifier [0x0x1307c94+229924]\n\tGetHandleVerifier [0x0x12f81f8+165768]\n\tGetHandleVerifier [0x0x12fecad+193085]\n\tGetHandleVerifier [0x0x12e8158+100072]\n\tGetHandleVerifier [0x0x12e82f0+100480]\n\tGetHandleVerifier [0x0x12d25aa+11066]\n\tBaseThreadInitThunk [0x0x775dfcc9+25]\n\tRtlGetAppContainerNamedObjectPath [0x0x777882ae+286]\n\tRtlGetAppContainerNamedObjectPath [0x0x7778827e+238]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "CATEGORIES = {\n",
    "    \"Chargers\": \"https://www.banggood.com/Wholesale-Chargers-ca-1055.html\",\n",
    "    \"RC Cars\": \"https://www.banggood.com/Wholesale-RC-Car-ca-7008.html\",\n",
    "    \"Multi Rotor Parts\": \"https://www.banggood.com/Wholesale-Multi-Rotor-Parts-ca-7014.html\",\n",
    "    \"Woodworking Clamp\": \"https://www.banggood.com/Wholesale-Woodworking-Clamp-ca-18883.html\",\n",
    "    \"Keyboard & Mouse\": \"https://www.banggood.com/Wholesale-Keyboards-and-Mouse-ca-5029.html\"\n",
    "}\n",
    "\n",
    "PAGES_TO_SCRAPE = 2\n",
    "\n",
    "# -----------------------------\n",
    "# DRIVER SETUP\n",
    "# -----------------------------\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    options.page_load_strategy = \"eager\"\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()), options=options\n",
    "    )\n",
    "    driver.set_page_load_timeout(25)\n",
    "    return driver\n",
    "\n",
    "# -----------------------------\n",
    "# EXTRACT PRODUCTS FROM PAGE\n",
    "# -----------------------------\n",
    "def extract_products(driver, category_name):\n",
    "    products = []\n",
    "\n",
    "    # Try multiple container selectors\n",
    "    possible_containers = [\"li.p-item\", \"div.p-wrap\", \"div.list-item\", \"li.gl-item\", \"div.product-item\"]\n",
    "    cards = []\n",
    "    for selector in possible_containers:\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        if cards:\n",
    "            break\n",
    "\n",
    "    if not cards:\n",
    "        print(\"    > No products found on this page.\")\n",
    "        return []\n",
    "\n",
    "    for card in cards:\n",
    "        try:\n",
    "            name = card.find_element(By.CSS_SELECTOR, \"a.title, a.p-title, .title a\").text.strip()\n",
    "        except:\n",
    "            name = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            price = card.find_element(By.CSS_SELECTOR, \"span.price-box, .price, .product-price\").text.strip()\n",
    "        except:\n",
    "            price = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            url = card.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "        except:\n",
    "            url = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            rating = card.find_element(By.CSS_SELECTOR, \"span.star\").get_attribute(\"data-score\")\n",
    "        except:\n",
    "            rating = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            reviews = card.find_element(By.CSS_SELECTOR, \"a.review\").text.strip()\n",
    "        except:\n",
    "            reviews = \"0 reviews\"\n",
    "\n",
    "        # Skip empty products\n",
    "        if name == \"N/A\" and price == \"N/A\":\n",
    "            continue\n",
    "\n",
    "        products.append({\n",
    "            \"Category\": category_name,\n",
    "            \"Name\": name,\n",
    "            \"Price\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Reviews\": reviews,\n",
    "            \"URL\": url\n",
    "        })\n",
    "\n",
    "    return products\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN SCRAPER FUNCTION\n",
    "# -----------------------------\n",
    "def scrape_banggood():\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "\n",
    "    print(\"Starting Banggood Scraper...\\n\")\n",
    "\n",
    "    for category_name, base_url in CATEGORIES.items():\n",
    "        print(f\"Scraping category: {category_name}\")\n",
    "\n",
    "        for page in range(1, PAGES_TO_SCRAPE + 1):\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "            print(f\"  - Page {page}...\", end=\" \")\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "            except TimeoutException:\n",
    "                driver.execute_script(\"window.stop();\")\n",
    "\n",
    "            # Scroll down to load products\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 3);\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 1.5);\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            products = extract_products(driver, category_name)\n",
    "            all_products.extend(products)\n",
    "            print(f\"Found {len(products)} items.\")\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save results\n",
    "    if all_products:\n",
    "        df = pd.DataFrame(all_products)\n",
    "        df.to_csv(\"banggood_products.csv\", index=False)\n",
    "        print(f\"\\nScraping complete! Total products: {len(all_products)}\")\n",
    "        print(\"Data saved to 'banggood_products.csv'\")\n",
    "    else:\n",
    "        print(\"No products scraped.\")\n",
    "\n",
    "# -----------------------------\n",
    "# RUN SCRIPT\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_banggood()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9097a5-88b4-4d06-86eb-8280d1819b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Scraper (Unlimited Pages Mode)...\n",
      "------------------------------------------\n",
      "Processing Category: Test & Measuring Module\n",
      "-> Found 30 items.1... \n",
      "  - Scraping Page 2... -> Found 60 items.\n",
      "  - Scraping Page 3... -> Found 60 items.\n",
      "  - Scraping Page 4... -> Found 18 items.\n",
      "  - Scraping Page 5... -> No products found. End of Category.\n",
      "Processing Category: USB Hubs\n",
      "-> Found 60 items.1... \n",
      "  - Scraping Page 2... -> Found 16 items.\n",
      "  - Scraping Page 3... -> No products found. End of Category.\n",
      "Processing Category: Smart Watch\n",
      "-> Found 30 items.1... \n",
      "  - Scraping Page 2... -> Found 60 items.\n",
      "  - Scraping Page 3... -> Found 19 items.\n",
      "  - Scraping Page 4... -> No products found. End of Category.\n",
      "Processing Category: Handheld Vacuum Cleaners\n",
      "-> Found 25 items.1... \n",
      "  - Scraping Page 2... -> No products found. End of Category.\n",
      "Processing Category: Router\n",
      "-> Found 30 items.1... \n",
      "  - Scraping Page 2... -> Found 30 items.\n",
      "  - Scraping Page 3... -> Found 60 items.\n",
      "  - Scraping Page 4... -> Found 30 items.\n",
      "  - Scraping Page 5... -> Found 60 items.\n",
      "  - Scraping Page 6... -> Found 30 items.\n",
      "  - Scraping Page 7... -> Found 60 items.\n",
      "  - Scraping Page 8... -> Found 60 items.\n",
      "  - Scraping Page 9... -> Found 22 items.\n",
      "  - Scraping Page 10... -> No products found. End of Category.\n",
      "-------------------\n",
      "Success! Total Scraped: 760 items.\n",
      "File saved: 'banggood_full_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchWindowException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- 1. PASTE YOUR LINKS HERE ---\n",
    "CATEGORIES = {\n",
    "    \"Test & Measuring Module\": \"https://www.banggood.com/Wholesale-Test-and-Measuring-Module-ca-2292.html\",\n",
    "    \"USB Hubs\": \"https://www.banggood.com/Wholesale-USB-Hubs-ca-5006.html\",\n",
    "    \"Smart Watch\": \"https://www.banggood.com/Wholesale-Smart-Watch-ca-2210.html\",\n",
    "    \"Handheld Vacuum Cleaners\": \"https://www.banggood.com/Wholesale-Handheld-Vacuum-Cleaners-c-13652.html\",\n",
    "    \"Router\": \"https://www.banggood.com/Wholesale-Router-Table-Plate-ca-18889.html\"\n",
    "}\n",
    "\n",
    "# --- SETTINGS ---\n",
    "MAX_PAGES_SAFETY = 50  # Safety limit (agar 50 se zyada pages ho to ruk jaye, infinite loop se bachne ke liye)\n",
    "\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    options.page_load_strategy = 'eager' \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_page_load_timeout(25)\n",
    "    return driver\n",
    "\n",
    "def extract_data(driver, category_name):\n",
    "    products_data = []\n",
    "    \n",
    "    # Smart Selectors: Try different patterns used by Banggood\n",
    "    selector_patterns = [\n",
    "        {\"item\": \"li.p-item\", \"name\": \"a.title\", \"price\": \"span.price-box\", \"url\": \"a.title\"},\n",
    "        {\"item\": \"div.p-wrap\", \"name\": \"a.p-title\", \"price\": \"span.price\", \"url\": \"a.p-title\"},\n",
    "        {\"item\": \"li.gl-item\", \"name\": \"a.title\", \"price\": \"span.price\", \"url\": \"a.title\"},\n",
    "        {\"item\": \"div.list-item\", \"name\": \"a.title\", \"price\": \"span.price\", \"url\": \"a.title\"}\n",
    "    ]\n",
    "    \n",
    "    active_pattern = None\n",
    "    product_cards = []\n",
    "\n",
    "    # Check which selector works on this page\n",
    "    for pattern in selector_patterns:\n",
    "        found_elements = driver.find_elements(By.CSS_SELECTOR, pattern[\"item\"])\n",
    "        if len(found_elements) > 0:\n",
    "            product_cards = found_elements\n",
    "            active_pattern = pattern\n",
    "            break\n",
    "    \n",
    "    if not product_cards:\n",
    "        return [] # Return empty if no products found\n",
    "\n",
    "    for card in product_cards:\n",
    "        try:\n",
    "            try: name = card.find_element(By.CSS_SELECTOR, active_pattern[\"name\"]).text.strip()\n",
    "            except: name = \"N/A\"\n",
    "\n",
    "            try: price = card.find_element(By.CSS_SELECTOR, active_pattern[\"price\"]).text.strip()\n",
    "            except: price = \"N/A\"\n",
    "\n",
    "            try: url = card.find_element(By.CSS_SELECTOR, active_pattern[\"url\"]).get_attribute(\"href\")\n",
    "            except: url = \"N/A\"\n",
    "\n",
    "            try: review_text = card.find_element(By.CSS_SELECTOR, \"a.review\").text.strip()\n",
    "            except: review_text = \"0 reviews\"\n",
    "            \n",
    "            try: rating = card.find_element(By.CSS_SELECTOR, \"span.star\").get_attribute(\"data-score\")\n",
    "            except: rating = \"N/A\"\n",
    "\n",
    "            products_data.append({\n",
    "                \"Category\": category_name,\n",
    "                \"Name\": name,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": review_text,\n",
    "                \"URL\": url\n",
    "            })\n",
    "        except Exception:\n",
    "            continue \n",
    "\n",
    "    return products_data\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "\n",
    "    print(\"Starting Scraper (Unlimited Pages Mode)...\")\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    for cat_name, base_url in CATEGORIES.items():\n",
    "        if \"PASTE_LINK\" in base_url:\n",
    "            print(f\"Skipping {cat_name} (No Link Provided)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing Category: {cat_name}\")\n",
    "        \n",
    "        page = 1\n",
    "        while page <= MAX_PAGES_SAFETY:\n",
    "            \n",
    "            # URL Construction for Pagination\n",
    "            if page == 1:\n",
    "                target_url = base_url\n",
    "            else:\n",
    "                target_url = f\"{base_url}?page={page}\"\n",
    "\n",
    "            print(f\"  - Scraping Page {page}...\", end=\" \")\n",
    "            \n",
    "            try:\n",
    "                try:\n",
    "                    driver.get(target_url)\n",
    "                except TimeoutException:\n",
    "                    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "                # Scroll Logic\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 2);\")\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # Extract\n",
    "                products = extract_data(driver, cat_name)\n",
    "                \n",
    "                # --- STOPPING CONDITION ---\n",
    "                if len(products) == 0:\n",
    "                    print(\"-> No products found. End of Category.\")\n",
    "                    break # Exit the loop for this category\n",
    "                \n",
    "                all_products.extend(products)\n",
    "                print(f\"-> Found {len(products)} items.\")\n",
    "                \n",
    "                page += 1 # Go to next page\n",
    "\n",
    "            except (NoSuchWindowException, WebDriverException) as e:\n",
    "                print(f\"\\n    !!! Browser Error. Restarting driver...\")\n",
    "                try: driver.quit()\n",
    "                except: pass\n",
    "                driver = setup_driver()\n",
    "                continue # Try the same page again or move on? Usually better to move on to avoid loops.\n",
    "                \n",
    "            time.sleep(1) \n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"-------------------\")\n",
    "    if all_products:\n",
    "        df = pd.DataFrame(all_products)\n",
    "        df.to_csv(\"banggood_full_data.csv\", index=False)\n",
    "        print(f\"Success! Total Scraped: {len(all_products)} items.\")\n",
    "        print(\"File saved: 'banggood_full_data.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped. Check links.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a84cb042-b76e-4e92-90a9-b4045b69bde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[→] Scraping category: RC Drones & Accessories\n",
      "  Scraping page 1: https://www.banggood.com/Toys-Hobbies-and-Robot/RC-Drones-and-Accessories-c-12179.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished RC Drones & Accessories: 0 products collected.\n",
      "\n",
      "[→] Scraping category: Smartphones\n",
      "  Scraping page 1: https://www.banggood.com/Consumer-Electronics/Mobile-Phones-c-10464.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished Smartphones: 0 products collected.\n",
      "\n",
      "[→] Scraping category: Laptops & Tablets\n",
      "  Scraping page 1: https://www.banggood.com/Computers-and-Office/Laptops-and-Tablets-c-10784.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished Laptops & Tablets: 0 products collected.\n",
      "\n",
      "[→] Scraping category: Tools & Home Improvement\n",
      "  Scraping page 1: https://www.banggood.com/Tools-and-Home-Improvement-c-12186.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished Tools & Home Improvement: 0 products collected.\n",
      "\n",
      "[→] Scraping category: LED Lights & Flashlights\n",
      "  Scraping page 1: https://www.banggood.com/Lights-and-Lighting/LED-Lights-and-Flashlights-c-12190.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished LED Lights & Flashlights: 0 products collected.\n",
      "\n",
      "No products scraped. Banggood may have updated structure or blocked requests.\n"
     ]
    }
   ],
   "source": [
    "# banggood_scraper.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "\n",
    "# ----------------------------- CONFIG -----------------------------\n",
    "# Be respectful! Don't hammer the server\n",
    "DELAY_BETWEEN_REQUESTS = random.uniform(2, 5)  # Random delay in seconds\n",
    "MAX_PAGES_PER_CATEGORY = 3  # Set to higher if needed (e.g., 10)\n",
    "\n",
    "# Use realistic headers\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"none\",\n",
    "}\n",
    "\n",
    "# Optional: Add proxies if you get blocked\n",
    "PROXIES = {\n",
    "    # \"http\": \"http://your_proxy:port\",\n",
    "    # \"https\": \"http://your_proxy:port\",\n",
    "}\n",
    "\n",
    "# 5 Popular Banggood categories (as of 2025)\n",
    "CATEGORIES = [\n",
    "    {\n",
    "        \"name\": \"RC Drones & Accessories\",\n",
    "        \"url\": \"https://www.banggood.com/Toys-Hobbies-and-Robot/RC-Drones-and-Accessories-c-12179.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Smartphones\",\n",
    "        \"url\": \"https://www.banggood.com/Consumer-Electronics/Mobile-Phones-c-10464.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Laptops & Tablets\",\n",
    "        \"url\": \"https://www.banggood.com/Computers-and-Office/Laptops-and-Tablets-c-10784.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tools & Home Improvement\",\n",
    "        \"url\": \"https://www.banggood.com/Tools-and-Home-Improvement-c-12186.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LED Lights & Flashlights\",\n",
    "        \"url\": \"https://www.banggood.com/Lights-and-Lighting/LED-Lights-and-Flashlights-c-12190.html\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Output file\n",
    "OUTPUT_CSV = \"banggood_products.csv\"\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Fetch page and return BeautifulSoup object\"\"\"\n",
    "    try:\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "        response = requests.get(url, headers=HEADERS, proxies=PROXIES, timeout=20)\n",
    "        \n",
    "        if response.status_code == 503 or \"cloudflare\" in response.text.lower():\n",
    "            print(f\"[!] Cloudflare protection detected on {url}\")\n",
    "            return None\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[!] Request failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_products_from_page(soup, category_name):\n",
    "    \"\"\"Extract product info from a single page\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    # Banggood product grid items (class changes sometimes, this works as of 2025)\n",
    "    product_items = soup.select('.product-item') or soup.select('.good-item') or soup.select('li[data-product-id]')\n",
    "    \n",
    "    print(f\"    Found {len(product_items)} products on this page\")\n",
    "    \n",
    "    for item in product_items:\n",
    "        try:\n",
    "            # Product name\n",
    "            title_tag = item.select_one('a.title, .title a, h3 a, .name a')\n",
    "            name = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "            product_url = urljoin(\"https://www.banggood.com\", title_tag['href']) if title_tag and title_tag.get('href') else \"N/A\"\n",
    "            \n",
    "            # Price\n",
    "            price_tag = item.select_one('.price, .price-new, .good-price, [class*=\"price\"]')\n",
    "            price = price_tag.get_text(strip=True).replace('US$', '').strip() if price_tag else \"N/A\"\n",
    "            \n",
    "            # Rating\n",
    "            rating_tag = item.select_one('.rating-num, .star-view span, .review-num')\n",
    "            rating = rating_tag.get_text(strip=True) if rating_tag else \"N/A\"\n",
    "            \n",
    "            # Reviews count\n",
    "            reviews_tag = item.select_one('.review-num, .review-count')\n",
    "            reviews = reviews_tag.get_text(strip=True).strip('() ') if reviews_tag else \"0\"\n",
    "            \n",
    "            products.append({\n",
    "                \"category\": category_name,\n",
    "                \"product_name\": name,\n",
    "                \"price_usd\": price,\n",
    "                \"rating\": rating,\n",
    "                \"reviews\": reviews,\n",
    "                \"product_url\": product_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"    Error parsing product: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "def scrape_category(category):\n",
    "    \"\"\"Scrape multiple pages of a category\"\"\"\n",
    "    all_products = []\n",
    "    base_url = category[\"url\"]\n",
    "    \n",
    "    print(f\"\\n[→] Scraping category: {category['name']}\")\n",
    "    \n",
    "    for page in range(1, MAX_PAGES_PER_CATEGORY + 1):\n",
    "        page_url = f\"{base_url}?p={page}\" if '?' in base_url else f\"{base_url}?p={page}\"\n",
    "        print(f\"  Scraping page {page}: {page_url}\")\n",
    "        \n",
    "        soup = get_soup(page_url)\n",
    "        if not soup:\n",
    "            print(f\"    Failed to retrieve page, possibly blocked.\")\n",
    "            break\n",
    "            \n",
    "        products = extract_products_from_page(soup, category[\"name\"])\n",
    "        if not products:\n",
    "            print(\"    No products found – selector may have changed or page is empty.\")\n",
    "            break\n",
    "            \n",
    "        all_products.extend(products)\n",
    "        print(f\"    → Collected {len(products)} products (Total: {len(all_products)})\")\n",
    "        \n",
    "        # Stop if no more products\n",
    "        if len(products) < 20:  # Banggood usually shows 60 per page\n",
    "            print(\"    Likely reached last page.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Finished {category['name']}: {len(all_products)} products collected.\")\n",
    "    return all_products\n",
    "\n",
    "def save_to_csv(all_data):\n",
    "    \"\"\"Save all scraped data to CSV\"\"\"\n",
    "    keys = [\"category\", \"product_name\", \"price_usd\", \"rating\", \"reviews\", \"product_url\"]\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "    print(f\"\\nSaved {len(all_data)} products to {OUTPUT_CSV}\")\n",
    "\n",
    "def main():\n",
    "    all_products = []\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        products = scrape_category(category)\n",
    "        all_products.extend(products)\n",
    "    \n",
    "    if all_products:\n",
    "        save_to_csv(all_products)\n",
    "        print(f\"\\nScraping completed! Total products: {len(all_products)}\")\n",
    "    else:\n",
    "        print(\"\\nNo products scraped. Banggood may have updated structure or blocked requests.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31438d0f-9dc5-4885-b337-f80ec02619bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category: Chargers\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: RC Cars\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: Multi Rotor Parts\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: Woodworking Clamp\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: Keyboard & Mouse\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "No data scraped. Likely JS-loaded content.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"Chargers\": \"https://www.banggood.com/Wholesale-Chargers-ca-1055.html\",\n",
    "    \"RC Cars\": \"https://www.banggood.com/Wholesale-RC-Car-ca-7008.html\",\n",
    "    \"Multi Rotor Parts\": \"https://www.banggood.com/Wholesale-Multi-Rotor-Parts-ca-7014.html\",\n",
    "    \"Woodworking Clamp\": \"https://www.banggood.com/Wholesale-Woodworking-Clamp-ca-18883.html\",\n",
    "    \"Keyboard & Mouse\": \"https://www.banggood.com/Wholesale-Keyboards-and-Mouse-ca-5029.html\"\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "PAGES_TO_SCRAPE = 2\n",
    "\n",
    "def scrape_page(url, category_name):\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    products_data = []\n",
    "\n",
    "    # Try multiple container classes (similar to Selenium logic)\n",
    "    containers = [\"li.p-item\", \"div.p-wrap\", \"div.list-item\", \"li.gl-item\", \"div.product-item\"]\n",
    "    cards = []\n",
    "    for c in containers:\n",
    "        found = soup.select(c)\n",
    "        if found:\n",
    "            cards = found\n",
    "            break\n",
    "\n",
    "    if not cards:\n",
    "        print(\"   > Warning: No product cards found (likely JS-loaded)\")\n",
    "        return []\n",
    "\n",
    "    for card in cards:\n",
    "        try:\n",
    "            name_tag = card.select_one(\"a.title, a.p-title, .title a\")\n",
    "            name = name_tag.get_text(strip=True) if name_tag else \"N/A\"\n",
    "            url = name_tag['href'] if name_tag and name_tag.has_attr('href') else \"N/A\"\n",
    "\n",
    "            price_tag = card.select_one(\"span.price-box, .price, .product-price\")\n",
    "            price = price_tag.get_text(strip=True) if price_tag else \"N/A\"\n",
    "\n",
    "            rating_tag = card.select_one(\"span.star\")\n",
    "            rating = rating_tag['data-score'] if rating_tag and rating_tag.has_attr('data-score') else \"N/A\"\n",
    "\n",
    "            reviews_tag = card.select_one(\"a.review\")\n",
    "            reviews = reviews_tag.get_text(strip=True) if reviews_tag else \"0 reviews\"\n",
    "\n",
    "            # Skip empty entries\n",
    "            if name == \"N/A\" and price == \"N/A\":\n",
    "                continue\n",
    "\n",
    "            products_data.append({\n",
    "                \"Category\": category_name,\n",
    "                \"Name\": name,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"URL\": url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing card:\", e)\n",
    "            continue\n",
    "\n",
    "    return products_data\n",
    "\n",
    "def main():\n",
    "    all_products = []\n",
    "\n",
    "    for cat_name, base_url in CATEGORIES.items():\n",
    "        print(f\"Processing Category: {cat_name}\")\n",
    "        for page in range(1, PAGES_TO_SCRAPE + 1):\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "            print(f\"  - Scraping Page {page}...\")\n",
    "            products = scrape_page(url, cat_name)\n",
    "            print(f\"    -> Found {len(products)} items.\")\n",
    "            all_products.extend(products)\n",
    "            time.sleep(1)  # polite delay\n",
    "\n",
    "    if all_products:\n",
    "        df = pd.DataFrame(all_products)\n",
    "        df.to_csv(\"banggood_products_bs.csv\", index=False)\n",
    "        print(f\"Success! Scraped {len(all_products)} items.\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"No data scraped. Likely JS-loaded content.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582555b-e96a-4e85-a3d3-6e51ca8bed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(\"banggood_products.csv\")\n",
    "\n",
    "# 2. Clean Price (Overwrite the 'Price' column directly)\n",
    "# Remove messy symbols: Â, £, $, US\n",
    "df[\"Price\"] = df[\"Price\"].astype(str).str.replace(\"Â\", \"\").str.replace(\"£\", \"\").str.replace(\"$\", \"\").str.replace(\"US\", \"\")\n",
    "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors='coerce')\n",
    "\n",
    "# 3. Clean Reviews (Overwrite the 'Reviews' column directly)\n",
    "# Remove \" reviews\" and brackets\n",
    "df[\"Reviews\"] = df[\"Reviews\"].astype(str).str.replace(\" reviews\", \"\").str.replace(\"(\", \"\").str.replace(\")\", \"\")\n",
    "df[\"Reviews\"] = pd.to_numeric(df[\"Reviews\"], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW STEP: DELETE THE ORIGINAL RATING COLUMN ---\n",
    "if \"Rating\" in df.columns:\n",
    "    df = df.drop(columns=['Rating'])\n",
    "    print(\"Rating column deleted successfully.\")\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 5. Create 2 New Features (Using the now clean columns)\n",
    "df[\"Total_Revenue\"] = df[\"Price\"] * df[\"Reviews\"]\n",
    "df[\"Is_Best_Seller\"] = df[\"Reviews\"] > 50 \n",
    "\n",
    "# 6. Save (Now the file won't have any messy columns)\n",
    "df.to_csv(\"banggood_products_cleaned.csv\", index=False)\n",
    "print(\"Done! Original columns are now clean.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78883e3b-2159-4b3d-9f78-21963649e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Headless Scraper (with Rating and Stock)...\n",
      "-----------------------------------------------\n",
      "Processing Category: Chargers\n",
      "-> Found 0 items. 1... \n",
      "  - Scraping Page 2... -> Found 0 items.\n",
      "Processing Category: RC Cars\n",
      "  - Scraping Page 1... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data scraped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 156\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[13], line 132\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m products \u001b[38;5;241m=\u001b[39m extract_data(driver, cat_name)\n\u001b[0;32m    133\u001b[0m all_products\u001b[38;5;241m.\u001b[39mextend(products)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-> Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(products)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m items.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 46\u001b[0m, in \u001b[0;36mextract_data\u001b[1;34m(driver, category_name)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m container \u001b[38;5;129;01min\u001b[39;00m possible_containers:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         cards \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     47\u001b[0m             EC\u001b[38;5;241m.\u001b[39mpresence_of_all_elements_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, container))\n\u001b[0;32m     48\u001b[0m         )\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:137\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchWindowException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CATEGORIES = {\n",
    "    \"Chargers\": \"https://www.banggood.com/Wholesale-Chargers-ca-1055.html\",\n",
    "    \"RC Cars\": \"https://www.banggood.com/Wholesale-RC-Car-ca-7008.html\",\n",
    "    \"Multi Rotor Parts\": \"https://www.banggood.com/Wholesale-Multi-Rotor-Parts-ca-7014.html\",\n",
    "    \"Woodworking Clamp\": \"https://www.banggood.com/Wholesale-Woodworking-Clamp-ca-18883.html\",\n",
    "    \"Keyboard & Mouse\": \"https://www.banggood.com/Wholesale-Keyboards-and-Mouse-ca-5029.html\"\n",
    "}\n",
    "PAGES_TO_SCRAPE = 2 \n",
    "\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    options.page_load_strategy = 'eager' \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_data(driver, category_name):\n",
    "    products_data = []\n",
    "    possible_containers = [\n",
    "        \"li.p-item\", \"div.p-wrap\", \"div.list-item\", \"li.gl-item\", \"div.product-item\", \"ul.p-list-items li\"\n",
    "    ]\n",
    "    cards = []\n",
    "    for container in possible_containers:\n",
    "        try:\n",
    "            cards = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container))\n",
    "            )\n",
    "            if len(cards) > 0:\n",
    "                break\n",
    "        except TimeoutException:\n",
    "            continue \n",
    "\n",
    "    if not cards:\n",
    "        return []\n",
    "\n",
    "    for card in cards:\n",
    "        name, price, url, rating, reviews, stock_status = \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"0 reviews\", \"Unknown\"\n",
    "        # 1. NAME\n",
    "        try:\n",
    "            name = card.find_element(By.CSS_SELECTOR, \"a.title, a.p-title\").text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        # 2. PRICE\n",
    "        try:\n",
    "            price = card.find_element(By.CSS_SELECTOR, \"span.price-box, .price, .product-price\").text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        # 3. URL\n",
    "        try:\n",
    "            url = card.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "        except:\n",
    "            pass\n",
    "        # 4. REVIEWS\n",
    "        try:\n",
    "            reviews = card.find_element(By.CSS_SELECTOR, \"a.review\").text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        # 5. RATING\n",
    "        try:\n",
    "            rating_element = card.find_element(By.CSS_SELECTOR, \"span.star, div.star, i.star\")\n",
    "            rating = rating_element.get_attribute(\"data-score\") or rating_element.get_attribute(\"title\")\n",
    "            if not rating or not re.match(r'^\\d+(\\.\\d+)?$', str(rating).strip()):\n",
    "                rating = \"N/A\"\n",
    "        except:\n",
    "            rating = \"N/A\"\n",
    "        # 6. STOCK AVAILABILITY\n",
    "        try:\n",
    "            stock_element = card.find_element(By.CSS_SELECTOR, \".stock, .availability\")\n",
    "            stock_text = stock_element.text.lower()\n",
    "            if \"in stock\" in stock_text:\n",
    "                stock_status = \"In Stock\"\n",
    "            elif \"out of stock\" in stock_text or \"sold out\" in stock_text:\n",
    "                stock_status = \"Out of Stock\"\n",
    "            else:\n",
    "                stock_status = stock_text\n",
    "        except:\n",
    "            stock_status = \"Unknown\"\n",
    "        # Only append if name or price is present\n",
    "        if name != \"N/A\" or price != \"N/A\":\n",
    "            products_data.append({\n",
    "                \"Category\": category_name,\n",
    "                \"Name\": name,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"Stock\": stock_status,\n",
    "                \"URL\": url\n",
    "            })\n",
    "    return products_data\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "    print(\"Starting Headless Scraper (with Rating and Stock)...\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    for cat_name, base_url in CATEGORIES.items():\n",
    "        print(f\"Processing Category: {cat_name}\")\n",
    "        for page in range(1, PAGES_TO_SCRAPE + 1):\n",
    "            if page == 1: target_url = base_url\n",
    "            else: target_url = f\"{base_url}?page={page}\"\n",
    "            print(f\"  - Scraping Page {page}...\", end=\" \")\n",
    "            try:\n",
    "                try:\n",
    "                    driver.get(target_url)\n",
    "                except TimeoutException:\n",
    "                    driver.execute_script(\"window.stop();\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 2);\")\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                products = extract_data(driver, cat_name)\n",
    "                all_products.extend(products)\n",
    "                print(f\"-> Found {len(products)} items.\")\n",
    "            except (NoSuchWindowException, WebDriverException):\n",
    "                print(f\"\\n    !!! Browser Error. Restarting driver...\")\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                driver = setup_driver()\n",
    "                continue\n",
    "            time.sleep(1) \n",
    "    driver.quit()\n",
    "    print(\"-------------------\")\n",
    "    if all_products:\n",
    "        df = pd.DataFrame(all_products)\n",
    "        df.to_csv(\"banggood_products.csv\", index=False)\n",
    "        print(f\"Success! Scraped {len(all_products)} items.\")\n",
    "        print(\"Data saved to 'banggood_products.csv'\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a23ca8c4-b6ba-4679-99d5-411f6bde167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Graph 5: Stock Availability Analysis (Count Plot) ---\n",
      "Error: 'Availability_Marker' column is missing. Please run the updated cleaning script first.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Availability_Marker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Availability_Marker'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m palette_colors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStandard Stock\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#007ACC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPre-Order/New Stock\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#FFC300\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOut of Stock\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#E35E5E\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Ensure the count plot is created correctly\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m marker_counts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvailability_Marker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m     31\u001b[0m ordered_markers \u001b[38;5;241m=\u001b[39m marker_counts\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     33\u001b[0m ax \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mcountplot(\n\u001b[0;32m     34\u001b[0m     data\u001b[38;5;241m=\u001b[39mdf, \n\u001b[0;32m     35\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvailability_Marker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# y-axis is the marker\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     order\u001b[38;5;241m=\u001b[39mordered_markers, \u001b[38;5;66;03m# Order by count\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     palette\u001b[38;5;241m=\u001b[39m[palette_colors\u001b[38;5;241m.\u001b[39mget(m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#999999\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m ordered_markers] \u001b[38;5;66;03m# Get colors based on marker name\u001b[39;00m\n\u001b[0;32m     38\u001b[0m )\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Availability_Marker'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set styling for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load Data (Using the cleaned file)\n",
    "try:\n",
    "    df = pd.read_csv(\"banggood_products_cleaned.csv\")\n",
    "    print(\"--- Graph 5: Stock Availability Analysis (Count Plot) ---\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'banggood_products_cleaned.csv' file not found. Please ensure the cleaning script has been run.\")\n",
    "    exit()\n",
    "    \n",
    "# Check if the Availability_Marker column exists and has been created by the cleaning script\n",
    "if 'Availability_Marker' not in df.columns:\n",
    "    print(\"Error: 'Availability_Marker' column is missing. Please run the updated cleaning script first.\")\n",
    "    exit()\n",
    "\n",
    "# GRAPH 5: Stock Availability Analysis (Count Plot)\n",
    "# This graph shows the proportion of products identified by the Availability_Marker.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors for better contrast\n",
    "palette_colors = {'Standard Stock': '#007ACC', 'Pre-Order/New Stock': '#FFC300', 'Out of Stock': '#E35E5E'}\n",
    "\n",
    "# Ensure the count plot is created correctly\n",
    "marker_counts = df['Availability_Marker'].value_counts()\n",
    "ordered_markers = marker_counts.index.tolist()\n",
    "\n",
    "ax = sns.countplot(\n",
    "    data=df, \n",
    "    y='Availability_Marker', # y-axis is the marker\n",
    "    order=ordered_markers, # Order by count\n",
    "    palette=[palette_colors.get(m, '#999999') for m in ordered_markers] # Get colors based on marker name\n",
    ")\n",
    "\n",
    "plt.title('Stock Availability vs. New/Pre-Order Products (Inferred from Name)', fontsize=16)\n",
    "plt.xlabel('Number of Products', fontsize=12)\n",
    "plt.ylabel('Availability Marker', fontsize=12)\n",
    "\n",
    "# Add percentage labels to the bars\n",
    "total = len(df)\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.1f}%'.format(100 * p.get_width() / total)\n",
    "    x = p.get_width()\n",
    "    y = p.get_y() + p.get_height() / 2\n",
    "    # Place text label to the right of the bar\n",
    "    ax.annotate(percentage, (x, y), ha='left', va='center', fontsize=10, xytext=(5, 0), textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd4cc5-6374-4381-a796-8b148a314480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

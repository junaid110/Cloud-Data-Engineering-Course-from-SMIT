{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d024f0d5-f9e0-429a-8f93-2319630bd6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Banggood Scraper...\n",
      "\n",
      "Scraping category: Chargers\n",
      "Found 26 items.\n",
      "  - Page 2... "
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.123)\nStacktrace:\n\tGetHandleVerifier [0x0x12dfe43+66515]\n\tGetHandleVerifier [0x0x12dfe84+66580]\n\t(No symbol) [0x0x10cdc48]\n\t(No symbol) [0x0x10ac18d]\n\t(No symbol) [0x0x1141a4e]\n\t(No symbol) [0x0x115c4d9]\n\t(No symbol) [0x0x113afc6]\n\t(No symbol) [0x0x110c2ca]\n\t(No symbol) [0x0x110d154]\n\tGetHandleVerifier [0x0x1537353+2521315]\n\tGetHandleVerifier [0x0x15322d3+2500707]\n\tGetHandleVerifier [0x0x1307c94+229924]\n\tGetHandleVerifier [0x0x12f81f8+165768]\n\tGetHandleVerifier [0x0x12fecad+193085]\n\tGetHandleVerifier [0x0x12e8158+100072]\n\tGetHandleVerifier [0x0x12e82f0+100480]\n\tGetHandleVerifier [0x0x12d25aa+11066]\n\tBaseThreadInitThunk [0x0x775dfcc9+25]\n\tRtlGetAppContainerNamedObjectPath [0x0x777882ae+286]\n\tRtlGetAppContainerNamedObjectPath [0x0x7778827e+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 153\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# RUN SCRIPT\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m     scrape_banggood()\n",
      "Cell \u001b[1;32mIn[12], line 120\u001b[0m, in \u001b[0;36mscrape_banggood\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n\u001b[0;32m    122\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.stop();\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:483\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:458\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    455\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor)\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    459\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mF:\\Softwares\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:233\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    231\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.123)\nStacktrace:\n\tGetHandleVerifier [0x0x12dfe43+66515]\n\tGetHandleVerifier [0x0x12dfe84+66580]\n\t(No symbol) [0x0x10cdc48]\n\t(No symbol) [0x0x10ac18d]\n\t(No symbol) [0x0x1141a4e]\n\t(No symbol) [0x0x115c4d9]\n\t(No symbol) [0x0x113afc6]\n\t(No symbol) [0x0x110c2ca]\n\t(No symbol) [0x0x110d154]\n\tGetHandleVerifier [0x0x1537353+2521315]\n\tGetHandleVerifier [0x0x15322d3+2500707]\n\tGetHandleVerifier [0x0x1307c94+229924]\n\tGetHandleVerifier [0x0x12f81f8+165768]\n\tGetHandleVerifier [0x0x12fecad+193085]\n\tGetHandleVerifier [0x0x12e8158+100072]\n\tGetHandleVerifier [0x0x12e82f0+100480]\n\tGetHandleVerifier [0x0x12d25aa+11066]\n\tBaseThreadInitThunk [0x0x775dfcc9+25]\n\tRtlGetAppContainerNamedObjectPath [0x0x777882ae+286]\n\tRtlGetAppContainerNamedObjectPath [0x0x7778827e+238]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "CATEGORIES = {\n",
    "    \"Chargers\": \"https://www.banggood.com/Wholesale-Chargers-ca-1055.html\",\n",
    "    \"RC Cars\": \"https://www.banggood.com/Wholesale-RC-Car-ca-7008.html\",\n",
    "    \"Multi Rotor Parts\": \"https://www.banggood.com/Wholesale-Multi-Rotor-Parts-ca-7014.html\",\n",
    "    \"Woodworking Clamp\": \"https://www.banggood.com/Wholesale-Woodworking-Clamp-ca-18883.html\",\n",
    "    \"Keyboard & Mouse\": \"https://www.banggood.com/Wholesale-Keyboards-and-Mouse-ca-5029.html\"\n",
    "}\n",
    "\n",
    "PAGES_TO_SCRAPE = 2\n",
    "\n",
    "# -----------------------------\n",
    "# DRIVER SETUP\n",
    "# -----------------------------\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    options.page_load_strategy = \"eager\"\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()), options=options\n",
    "    )\n",
    "    driver.set_page_load_timeout(25)\n",
    "    return driver\n",
    "\n",
    "# -----------------------------\n",
    "# EXTRACT PRODUCTS FROM PAGE\n",
    "# -----------------------------\n",
    "def extract_products(driver, category_name):\n",
    "    products = []\n",
    "\n",
    "    # Try multiple container selectors\n",
    "    possible_containers = [\"li.p-item\", \"div.p-wrap\", \"div.list-item\", \"li.gl-item\", \"div.product-item\"]\n",
    "    cards = []\n",
    "    for selector in possible_containers:\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        if cards:\n",
    "            break\n",
    "\n",
    "    if not cards:\n",
    "        print(\"    > No products found on this page.\")\n",
    "        return []\n",
    "\n",
    "    for card in cards:\n",
    "        try:\n",
    "            name = card.find_element(By.CSS_SELECTOR, \"a.title, a.p-title, .title a\").text.strip()\n",
    "        except:\n",
    "            name = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            price = card.find_element(By.CSS_SELECTOR, \"span.price-box, .price, .product-price\").text.strip()\n",
    "        except:\n",
    "            price = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            url = card.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "        except:\n",
    "            url = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            rating = card.find_element(By.CSS_SELECTOR, \"span.star\").get_attribute(\"data-score\")\n",
    "        except:\n",
    "            rating = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            reviews = card.find_element(By.CSS_SELECTOR, \"a.review\").text.strip()\n",
    "        except:\n",
    "            reviews = \"0 reviews\"\n",
    "\n",
    "        # Skip empty products\n",
    "        if name == \"N/A\" and price == \"N/A\":\n",
    "            continue\n",
    "\n",
    "        products.append({\n",
    "            \"Category\": category_name,\n",
    "            \"Name\": name,\n",
    "            \"Price\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Reviews\": reviews,\n",
    "            \"URL\": url\n",
    "        })\n",
    "\n",
    "    return products\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN SCRAPER FUNCTION\n",
    "# -----------------------------\n",
    "def scrape_banggood():\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "\n",
    "    print(\"Starting Banggood Scraper...\\n\")\n",
    "\n",
    "    for category_name, base_url in CATEGORIES.items():\n",
    "        print(f\"Scraping category: {category_name}\")\n",
    "\n",
    "        for page in range(1, PAGES_TO_SCRAPE + 1):\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "            print(f\"  - Page {page}...\", end=\" \")\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "            except TimeoutException:\n",
    "                driver.execute_script(\"window.stop();\")\n",
    "\n",
    "            # Scroll down to load products\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 3);\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 1.5);\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            products = extract_products(driver, category_name)\n",
    "            all_products.extend(products)\n",
    "            print(f\"Found {len(products)} items.\")\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save results\n",
    "    if all_products:\n",
    "        df = pd.DataFrame(all_products)\n",
    "        df.to_csv(\"banggood_products.csv\", index=False)\n",
    "        print(f\"\\nScraping complete! Total products: {len(all_products)}\")\n",
    "        print(\"Data saved to 'banggood_products.csv'\")\n",
    "    else:\n",
    "        print(\"No products scraped.\")\n",
    "\n",
    "# -----------------------------\n",
    "# RUN SCRIPT\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_banggood()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9097a5-88b4-4d06-86eb-8280d1819b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Scraper (Unlimited Pages Mode)...\n",
      "------------------------------------------\n",
      "Processing Category: Test & Measuring Module\n",
      "-> Found 30 items.1... \n",
      "  - Scraping Page 2... -> Found 60 items.\n",
      "  - Scraping Page 3... -> Found 60 items.\n",
      "  - Scraping Page 4... -> Found 18 items.\n",
      "  - Scraping Page 5... -> No products found. End of Category.\n",
      "Processing Category: USB Hubs\n",
      "-> Found 60 items.1... \n",
      "  - Scraping Page 2... -> Found 16 items.\n",
      "  - Scraping Page 3... -> No products found. End of Category.\n",
      "Processing Category: Smart Watch\n",
      "-> Found 30 items.1... \n",
      "  - Scraping Page 2... -> Found 60 items.\n",
      "  - Scraping Page 3... -> Found 19 items.\n",
      "  - Scraping Page 4... -> No products found. End of Category.\n",
      "Processing Category: Handheld Vacuum Cleaners\n",
      "-> Found 25 items.1... \n",
      "  - Scraping Page 2... -> No products found. End of Category.\n",
      "Processing Category: Router\n",
      "-> Found 30 items.1... \n",
      "  - Scraping Page 2... -> Found 30 items.\n",
      "  - Scraping Page 3... -> Found 60 items.\n",
      "  - Scraping Page 4... -> Found 30 items.\n",
      "  - Scraping Page 5... -> Found 60 items.\n",
      "  - Scraping Page 6... -> Found 30 items.\n",
      "  - Scraping Page 7... -> Found 60 items.\n",
      "  - Scraping Page 8... -> Found 60 items.\n",
      "  - Scraping Page 9... -> Found 22 items.\n",
      "  - Scraping Page 10... -> No products found. End of Category.\n",
      "-------------------\n",
      "Success! Total Scraped: 760 items.\n",
      "File saved: 'banggood_full_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchWindowException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- 1. PASTE YOUR LINKS HERE ---\n",
    "CATEGORIES = {\n",
    "    \"Test & Measuring Module\": \"https://www.banggood.com/Wholesale-Test-and-Measuring-Module-ca-2292.html\",\n",
    "    \"USB Hubs\": \"https://www.banggood.com/Wholesale-USB-Hubs-ca-5006.html\",\n",
    "    \"Smart Watch\": \"https://www.banggood.com/Wholesale-Smart-Watch-ca-2210.html\",\n",
    "    \"Handheld Vacuum Cleaners\": \"https://www.banggood.com/Wholesale-Handheld-Vacuum-Cleaners-c-13652.html\",\n",
    "    \"Router\": \"https://www.banggood.com/Wholesale-Router-Table-Plate-ca-18889.html\"\n",
    "}\n",
    "\n",
    "# --- SETTINGS ---\n",
    "MAX_PAGES_SAFETY = 50  # Safety limit (agar 50 se zyada pages ho to ruk jaye, infinite loop se bachne ke liye)\n",
    "\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    options.page_load_strategy = 'eager' \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_page_load_timeout(25)\n",
    "    return driver\n",
    "\n",
    "def extract_data(driver, category_name):\n",
    "    products_data = []\n",
    "    \n",
    "    # Smart Selectors: Try different patterns used by Banggood\n",
    "    selector_patterns = [\n",
    "        {\"item\": \"li.p-item\", \"name\": \"a.title\", \"price\": \"span.price-box\", \"url\": \"a.title\"},\n",
    "        {\"item\": \"div.p-wrap\", \"name\": \"a.p-title\", \"price\": \"span.price\", \"url\": \"a.p-title\"},\n",
    "        {\"item\": \"li.gl-item\", \"name\": \"a.title\", \"price\": \"span.price\", \"url\": \"a.title\"},\n",
    "        {\"item\": \"div.list-item\", \"name\": \"a.title\", \"price\": \"span.price\", \"url\": \"a.title\"}\n",
    "    ]\n",
    "    \n",
    "    active_pattern = None\n",
    "    product_cards = []\n",
    "\n",
    "    # Check which selector works on this page\n",
    "    for pattern in selector_patterns:\n",
    "        found_elements = driver.find_elements(By.CSS_SELECTOR, pattern[\"item\"])\n",
    "        if len(found_elements) > 0:\n",
    "            product_cards = found_elements\n",
    "            active_pattern = pattern\n",
    "            break\n",
    "    \n",
    "    if not product_cards:\n",
    "        return [] # Return empty if no products found\n",
    "\n",
    "    for card in product_cards:\n",
    "        try:\n",
    "            try: name = card.find_element(By.CSS_SELECTOR, active_pattern[\"name\"]).text.strip()\n",
    "            except: name = \"N/A\"\n",
    "\n",
    "            try: price = card.find_element(By.CSS_SELECTOR, active_pattern[\"price\"]).text.strip()\n",
    "            except: price = \"N/A\"\n",
    "\n",
    "            try: url = card.find_element(By.CSS_SELECTOR, active_pattern[\"url\"]).get_attribute(\"href\")\n",
    "            except: url = \"N/A\"\n",
    "\n",
    "            try: review_text = card.find_element(By.CSS_SELECTOR, \"a.review\").text.strip()\n",
    "            except: review_text = \"0 reviews\"\n",
    "            \n",
    "            try: rating = card.find_element(By.CSS_SELECTOR, \"span.star\").get_attribute(\"data-score\")\n",
    "            except: rating = \"N/A\"\n",
    "\n",
    "            products_data.append({\n",
    "                \"Category\": category_name,\n",
    "                \"Name\": name,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": review_text,\n",
    "                \"URL\": url\n",
    "            })\n",
    "        except Exception:\n",
    "            continue \n",
    "\n",
    "    return products_data\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "\n",
    "    print(\"Starting Scraper (Unlimited Pages Mode)...\")\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    for cat_name, base_url in CATEGORIES.items():\n",
    "        if \"PASTE_LINK\" in base_url:\n",
    "            print(f\"Skipping {cat_name} (No Link Provided)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing Category: {cat_name}\")\n",
    "        \n",
    "        page = 1\n",
    "        while page <= MAX_PAGES_SAFETY:\n",
    "            \n",
    "            # URL Construction for Pagination\n",
    "            if page == 1:\n",
    "                target_url = base_url\n",
    "            else:\n",
    "                target_url = f\"{base_url}?page={page}\"\n",
    "\n",
    "            print(f\"  - Scraping Page {page}...\", end=\" \")\n",
    "            \n",
    "            try:\n",
    "                try:\n",
    "                    driver.get(target_url)\n",
    "                except TimeoutException:\n",
    "                    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "                # Scroll Logic\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 2);\")\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # Extract\n",
    "                products = extract_data(driver, cat_name)\n",
    "                \n",
    "                # --- STOPPING CONDITION ---\n",
    "                if len(products) == 0:\n",
    "                    print(\"-> No products found. End of Category.\")\n",
    "                    break # Exit the loop for this category\n",
    "                \n",
    "                all_products.extend(products)\n",
    "                print(f\"-> Found {len(products)} items.\")\n",
    "                \n",
    "                page += 1 # Go to next page\n",
    "\n",
    "            except (NoSuchWindowException, WebDriverException) as e:\n",
    "                print(f\"\\n    !!! Browser Error. Restarting driver...\")\n",
    "                try: driver.quit()\n",
    "                except: pass\n",
    "                driver = setup_driver()\n",
    "                continue # Try the same page again or move on? Usually better to move on to avoid loops.\n",
    "                \n",
    "            time.sleep(1) \n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"-------------------\")\n",
    "    if all_products:\n",
    "        df = pd.DataFrame(all_products)\n",
    "        df.to_csv(\"banggood_full_data.csv\", index=False)\n",
    "        print(f\"Success! Total Scraped: {len(all_products)} items.\")\n",
    "        print(\"File saved: 'banggood_full_data.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped. Check links.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a84cb042-b76e-4e92-90a9-b4045b69bde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[→] Scraping category: RC Drones & Accessories\n",
      "  Scraping page 1: https://www.banggood.com/Toys-Hobbies-and-Robot/RC-Drones-and-Accessories-c-12179.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished RC Drones & Accessories: 0 products collected.\n",
      "\n",
      "[→] Scraping category: Smartphones\n",
      "  Scraping page 1: https://www.banggood.com/Consumer-Electronics/Mobile-Phones-c-10464.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished Smartphones: 0 products collected.\n",
      "\n",
      "[→] Scraping category: Laptops & Tablets\n",
      "  Scraping page 1: https://www.banggood.com/Computers-and-Office/Laptops-and-Tablets-c-10784.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished Laptops & Tablets: 0 products collected.\n",
      "\n",
      "[→] Scraping category: Tools & Home Improvement\n",
      "  Scraping page 1: https://www.banggood.com/Tools-and-Home-Improvement-c-12186.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished Tools & Home Improvement: 0 products collected.\n",
      "\n",
      "[→] Scraping category: LED Lights & Flashlights\n",
      "  Scraping page 1: https://www.banggood.com/Lights-and-Lighting/LED-Lights-and-Flashlights-c-12190.html?p=1\n",
      "    Found 0 products on this page\n",
      "    No products found – selector may have changed or page is empty.\n",
      "Finished LED Lights & Flashlights: 0 products collected.\n",
      "\n",
      "No products scraped. Banggood may have updated structure or blocked requests.\n"
     ]
    }
   ],
   "source": [
    "# banggood_scraper.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "\n",
    "# ----------------------------- CONFIG -----------------------------\n",
    "# Be respectful! Don't hammer the server\n",
    "DELAY_BETWEEN_REQUESTS = random.uniform(2, 5)  # Random delay in seconds\n",
    "MAX_PAGES_PER_CATEGORY = 3  # Set to higher if needed (e.g., 10)\n",
    "\n",
    "# Use realistic headers\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"none\",\n",
    "}\n",
    "\n",
    "# Optional: Add proxies if you get blocked\n",
    "PROXIES = {\n",
    "    # \"http\": \"http://your_proxy:port\",\n",
    "    # \"https\": \"http://your_proxy:port\",\n",
    "}\n",
    "\n",
    "# 5 Popular Banggood categories (as of 2025)\n",
    "CATEGORIES = [\n",
    "    {\n",
    "        \"name\": \"RC Drones & Accessories\",\n",
    "        \"url\": \"https://www.banggood.com/Toys-Hobbies-and-Robot/RC-Drones-and-Accessories-c-12179.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Smartphones\",\n",
    "        \"url\": \"https://www.banggood.com/Consumer-Electronics/Mobile-Phones-c-10464.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Laptops & Tablets\",\n",
    "        \"url\": \"https://www.banggood.com/Computers-and-Office/Laptops-and-Tablets-c-10784.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tools & Home Improvement\",\n",
    "        \"url\": \"https://www.banggood.com/Tools-and-Home-Improvement-c-12186.html\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LED Lights & Flashlights\",\n",
    "        \"url\": \"https://www.banggood.com/Lights-and-Lighting/LED-Lights-and-Flashlights-c-12190.html\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Output file\n",
    "OUTPUT_CSV = \"banggood_products.csv\"\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Fetch page and return BeautifulSoup object\"\"\"\n",
    "    try:\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "        response = requests.get(url, headers=HEADERS, proxies=PROXIES, timeout=20)\n",
    "        \n",
    "        if response.status_code == 503 or \"cloudflare\" in response.text.lower():\n",
    "            print(f\"[!] Cloudflare protection detected on {url}\")\n",
    "            return None\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[!] Request failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_products_from_page(soup, category_name):\n",
    "    \"\"\"Extract product info from a single page\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    # Banggood product grid items (class changes sometimes, this works as of 2025)\n",
    "    product_items = soup.select('.product-item') or soup.select('.good-item') or soup.select('li[data-product-id]')\n",
    "    \n",
    "    print(f\"    Found {len(product_items)} products on this page\")\n",
    "    \n",
    "    for item in product_items:\n",
    "        try:\n",
    "            # Product name\n",
    "            title_tag = item.select_one('a.title, .title a, h3 a, .name a')\n",
    "            name = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "            product_url = urljoin(\"https://www.banggood.com\", title_tag['href']) if title_tag and title_tag.get('href') else \"N/A\"\n",
    "            \n",
    "            # Price\n",
    "            price_tag = item.select_one('.price, .price-new, .good-price, [class*=\"price\"]')\n",
    "            price = price_tag.get_text(strip=True).replace('US$', '').strip() if price_tag else \"N/A\"\n",
    "            \n",
    "            # Rating\n",
    "            rating_tag = item.select_one('.rating-num, .star-view span, .review-num')\n",
    "            rating = rating_tag.get_text(strip=True) if rating_tag else \"N/A\"\n",
    "            \n",
    "            # Reviews count\n",
    "            reviews_tag = item.select_one('.review-num, .review-count')\n",
    "            reviews = reviews_tag.get_text(strip=True).strip('() ') if reviews_tag else \"0\"\n",
    "            \n",
    "            products.append({\n",
    "                \"category\": category_name,\n",
    "                \"product_name\": name,\n",
    "                \"price_usd\": price,\n",
    "                \"rating\": rating,\n",
    "                \"reviews\": reviews,\n",
    "                \"product_url\": product_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"    Error parsing product: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "def scrape_category(category):\n",
    "    \"\"\"Scrape multiple pages of a category\"\"\"\n",
    "    all_products = []\n",
    "    base_url = category[\"url\"]\n",
    "    \n",
    "    print(f\"\\n[→] Scraping category: {category['name']}\")\n",
    "    \n",
    "    for page in range(1, MAX_PAGES_PER_CATEGORY + 1):\n",
    "        page_url = f\"{base_url}?p={page}\" if '?' in base_url else f\"{base_url}?p={page}\"\n",
    "        print(f\"  Scraping page {page}: {page_url}\")\n",
    "        \n",
    "        soup = get_soup(page_url)\n",
    "        if not soup:\n",
    "            print(f\"    Failed to retrieve page, possibly blocked.\")\n",
    "            break\n",
    "            \n",
    "        products = extract_products_from_page(soup, category[\"name\"])\n",
    "        if not products:\n",
    "            print(\"    No products found – selector may have changed or page is empty.\")\n",
    "            break\n",
    "            \n",
    "        all_products.extend(products)\n",
    "        print(f\"    → Collected {len(products)} products (Total: {len(all_products)})\")\n",
    "        \n",
    "        # Stop if no more products\n",
    "        if len(products) < 20:  # Banggood usually shows 60 per page\n",
    "            print(\"    Likely reached last page.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Finished {category['name']}: {len(all_products)} products collected.\")\n",
    "    return all_products\n",
    "\n",
    "def save_to_csv(all_data):\n",
    "    \"\"\"Save all scraped data to CSV\"\"\"\n",
    "    keys = [\"category\", \"product_name\", \"price_usd\", \"rating\", \"reviews\", \"product_url\"]\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "    print(f\"\\nSaved {len(all_data)} products to {OUTPUT_CSV}\")\n",
    "\n",
    "def main():\n",
    "    all_products = []\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        products = scrape_category(category)\n",
    "        all_products.extend(products)\n",
    "    \n",
    "    if all_products:\n",
    "        save_to_csv(all_products)\n",
    "        print(f\"\\nScraping completed! Total products: {len(all_products)}\")\n",
    "    else:\n",
    "        print(\"\\nNo products scraped. Banggood may have updated structure or blocked requests.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31438d0f-9dc5-4885-b337-f80ec02619bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category: Chargers\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: RC Cars\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: Multi Rotor Parts\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: Woodworking Clamp\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "Processing Category: Keyboard & Mouse\n",
      "  - Scraping Page 1...\n",
      "    -> Found 0 items.\n",
      "  - Scraping Page 2...\n",
      "    -> Found 0 items.\n",
      "No data scraped. Likely JS-loaded content.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"Chargers\": \"https://www.banggood.com/Wholesale-Chargers-ca-1055.html\",\n",
    "    \"RC Cars\": \"https://www.banggood.com/Wholesale-RC-Car-ca-7008.html\",\n",
    "    \"Multi Rotor Parts\": \"https://www.banggood.com/Wholesale-Multi-Rotor-Parts-ca-7014.html\",\n",
    "    \"Woodworking Clamp\": \"https://www.banggood.com/Wholesale-Woodworking-Clamp-ca-18883.html\",\n",
    "    \"Keyboard & Mouse\": \"https://www.banggood.com/Wholesale-Keyboards-and-Mouse-ca-5029.html\"\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "PAGES_TO_SCRAPE = 2\n",
    "\n",
    "def scrape_page(url, category_name):\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    products_data = []\n",
    "\n",
    "    # Try multiple container classes (similar to Selenium logic)\n",
    "    containers = [\"li.p-item\", \"div.p-wrap\", \"div.list-item\", \"li.gl-item\", \"div.product-item\"]\n",
    "    cards = []\n",
    "    for c in containers:\n",
    "        found = soup.select(c)\n",
    "        if found:\n",
    "            cards = found\n",
    "            break\n",
    "\n",
    "    if not cards:\n",
    "        print(\"   > Warning: No product cards found (likely JS-loaded)\")\n",
    "        return []\n",
    "\n",
    "    for card in cards:\n",
    "        try:\n",
    "            name_tag = card.select_one(\"a.title, a.p-title, .title a\")\n",
    "            name = name_tag.get_text(strip=True) if name_tag else \"N/A\"\n",
    "            url = name_tag['href'] if name_tag and name_tag.has_attr('href') else \"N/A\"\n",
    "\n",
    "            price_tag = card.select_one(\"span.price-box, .price, .product-price\")\n",
    "            price = price_tag.get_text(strip=True) if price_tag else \"N/A\"\n",
    "\n",
    "            rating_tag = card.select_one(\"span.star\")\n",
    "            rating = rating_tag['data-score'] if rating_tag and rating_tag.has_attr('data-score') else \"N/A\"\n",
    "\n",
    "            reviews_tag = card.select_one(\"a.review\")\n",
    "            reviews = reviews_tag.get_text(strip=True) if reviews_tag else \"0 reviews\"\n",
    "\n",
    "            # Skip empty entries\n",
    "            if name == \"N/A\" and price == \"N/A\":\n",
    "                continue\n",
    "\n",
    "            products_data.append({\n",
    "                \"Category\": category_name,\n",
    "                \"Name\": name,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"URL\": url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing card:\", e)\n",
    "            continue\n",
    "\n",
    "    return products_data\n",
    "\n",
    "def main():\n",
    "    all_products = []\n",
    "\n",
    "    for cat_name, base_url in CATEGORIES.items():\n",
    "        print(f\"Processing Category: {cat_name}\")\n",
    "        for page in range(1, PAGES_TO_SCRAPE + 1):\n",
    "            url = base_url if page == 1 else f\"{base_url}?page={page}\"\n",
    "            print(f\"  - Scraping Page {page}...\")\n",
    "            products = scrape_page(url, cat_name)\n",
    "            print(f\"    -> Found {len(products)} items.\")\n",
    "            all_products.extend(products)\n",
    "            time.sleep(1)  # polite delay\n",
    "\n",
    "    if all_products:\n",
    "        df = pd.DataFrame(all_products)\n",
    "        df.to_csv(\"banggood_products_bs.csv\", index=False)\n",
    "        print(f\"Success! Scraped {len(all_products)} items.\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"No data scraped. Likely JS-loaded content.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582555b-e96a-4e85-a3d3-6e51ca8bed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(\"banggood_products.csv\")\n",
    "\n",
    "# 2. Clean Price (Overwrite the 'Price' column directly)\n",
    "# Remove messy symbols: Â, £, $, US\n",
    "df[\"Price\"] = df[\"Price\"].astype(str).str.replace(\"Â\", \"\").str.replace(\"£\", \"\").str.replace(\"$\", \"\").str.replace(\"US\", \"\")\n",
    "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors='coerce')\n",
    "\n",
    "# 3. Clean Reviews (Overwrite the 'Reviews' column directly)\n",
    "# Remove \" reviews\" and brackets\n",
    "df[\"Reviews\"] = df[\"Reviews\"].astype(str).str.replace(\" reviews\", \"\").str.replace(\"(\", \"\").str.replace(\")\", \"\")\n",
    "df[\"Reviews\"] = pd.to_numeric(df[\"Reviews\"], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW STEP: DELETE THE ORIGINAL RATING COLUMN ---\n",
    "if \"Rating\" in df.columns:\n",
    "    df = df.drop(columns=['Rating'])\n",
    "    print(\"Rating column deleted successfully.\")\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 5. Create 2 New Features (Using the now clean columns)\n",
    "df[\"Total_Revenue\"] = df[\"Price\"] * df[\"Reviews\"]\n",
    "df[\"Is_Best_Seller\"] = df[\"Reviews\"] > 50 \n",
    "\n",
    "# 6. Save (Now the file won't have any messy columns)\n",
    "df.to_csv(\"banggood_products_cleaned.csv\", index=False)\n",
    "print(\"Done! Original columns are now clean.\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
